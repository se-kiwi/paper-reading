# xPU， 各种处理器单元

## 各种处理器单元简介

### CPU

CPU是最经典的运算单元，其实际上是一块图灵完备的冯诺依曼结构运算器，具有三个组件：
- 逻辑运算组件：ALU， 
- 存储组件： Cache
- 控制部件：控制电路

![CPU体系结构](/02.TechnicalReport/xPU_assests/cpu_architecture.png)

### GPU

GPU是专门强化并行计算能力的运算单元，其不能单独工作，必须配备CPU，但GPU与CPU有以下不同：
- ALU显著多于CPU，但cache很少
- 缓存用来合并访问，而非为locality服务
- 控制单元用来合并访问而非逻辑控制
- 指令延迟大于CPU，但通过极高的吞吐量平摊了延迟


![GPU体系结构](/02.TechnicalReport/xPU_assests/gpu_architecture.png)

### TPU

TPU，Tensor Processing Unit，即张量处理器。是由谷歌推出的用于加速人工智能的专用芯片。
- TPU1.0在加速NN（Neural network）上，比当前GPU、CPU快15~30倍，性能功耗比高出30~80倍，而最新的TPU3.0性能可达100PFLOPS。
- 采用GPU常用的GDDR5存储器能使性能TPOS指标再高3倍，并将能效比指标 TOPS/Watt 提高到GPU的70倍，CPU的200倍。
- TPU使用了更大的片上内存，以此避免片外内存访问来适应NN计算的特性。
- TPU的高性能还来自于对低运算精度的容忍，由于低精度计算带来的算法准确度损失小，所以可以通过允许低精度计算带来更低的功耗、更快的速度、更小的带宽等优点。
- TPU采用脉动阵列架构处理卷积，在这一特定workload上效率很高，但在其他workload上效率可能不高。

![TPU体系结构](/02.TechnicalReport/xPU_assests/tpu_architecture.jpeg)
 
### NPU

NPU，Neural network Processing Unit，即神经网络处理器。是用电路来加速神经元网络，对神经网络的核心算法做针对性优化，并以此来实现人工神经网络并实现相关功能。相关芯片有国内的寒武纪芯片、IBM的TrueNorth和英特尔将要推出的神经网络处理器Nervana NNP L-1000。
- 以寒武纪的DianNao为例，主频为0.98GHz，峰值性能达每秒4520亿次神经网络基本运算，65nm工艺下功耗为0.485W，面积3.02平方毫米mm。
- 华为推出的麒麟970就集成了寒武纪的NPU，以此实现了图片优化，人脸识别，姿态识别等功能。
- 寒武纪还推出了针对神经网络的指令集——DianNaoYu。

### 其他处理器
诸如，华为的基于达芬奇架构的昇腾AI芯片、高通的骁龙855搭载的NPU芯片、苹果的专用芯片Apple Neural Engine、中星微的星光智能一号，比特大陆推出的ASIC芯片等。

## 各种处理单元总结

### 算力

#### 训练端
- 目前人工智能训练使用的运算单元主要有三种，GPU、TPU以及CPU。
- TPU由谷歌拥有全部知识产权，但由于不向外大批量出售，所以仅用于其内部使用。其初代产品不能用于训练，后续产品对人工智能的训练针对性进行了优化。
- 训练使用的GPU主要是NVIDIA公司的产品，由于其先发优势和建立于此迅速发展的环境，便于使用的CUDA等，NVIDIA的GPU在AI的训练端占据统治地位。
- CPU主要有Intel和AMD两家公司，其中Intel推出了caffe框架，并针对自家芯片做了优化，同时提供了内核库和拓展指令集，使得其芯片训练人工智能的效率与GPU的差距显著减小。

#### 预测端
- 在人工智能预测阶段使用的运算单元主要有四种，GPU、CPU、FPGA、TPU。
- TPU针对人工智能预测阶段专门做了优化，使得其能对人工智能的算法针对性地加速，由于Google在其设计上就倾向于支持TensorFlow框架，所以在相关方面性能尤为卓著。
- GPU与CPU在预测阶段基本没有区别，而由于预测阶段目前要求还不算太高，相关技术不太成熟，所以除了GPU比CPU快一些之外，没有太大区别。
- FPGA是针对已经生成好的模型做针对性优化，以此来达到性能的提升，微软、亚马逊等云服务商除了采用GPU、TPU，也采用FPGA进行网络加速。

#### 移动端
- 移动端主要是NPU居多，其主要功能比较简单，只有辅助拍照、姿态识别、人脸识别等功能，主要是图像识别领域的优化。
- 华为在荣耀、mate等系列手机采用寒武纪和自主研发的NPU来优化手机功能。
- 苹果在A12等芯片上搭载自主研发的NPU，Neural Engine来优化拍照等功能。
- 高通也推出了自己的人工智能引擎，包括了神经处理引擎，神经网络相关软件但不包括硬件实现的NPU。

### 通用性
- 在各种芯片之中，只有CPU和GPU是通用芯片，而只有CPU是图灵完备的，通用性最强。
- 在算力方面，明显的，算力越强的越是趋向于专用芯片，而目前各个厂家热衷的“人工智能芯片”也大多都是ASIC（专用集成电路），在算力和通用性之间存在明显的trade-off。
- 一般来说，在神经网络的训练阶段，主要还是使用通用性稍微好一些的硬件，所以云端的训练用芯片主要是NVIDIA的GPU，Intel、IBM的CPU、Google的TPU、苹果自研的CPU，此外还有AMD、ARM、高通的CPU。
- 在神经网络的预测阶段，一般采用边缘计算的方式，将一些运算下放到用户端，主要体现在移动设备的专用神经网络芯片，也就是NPU。在这方面，国内的寒武纪、华为都取得了很不错的成就，安卓厂商目前搭载NPU的还不多，大多使用手机的ARM芯片进行运算，但是相对于NPU性能不够好。
- FPGA作为现场可编程门电路阵列，在数学上也是图灵完备的，但由于其电路可编程，所以就可以针对算法进行特别优化，所以一般来说其应用范围更加宽阔，经常被用来针对算法中的某一部分进行优化。

